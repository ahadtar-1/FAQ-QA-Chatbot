"""
The module comprises of the functions used in the API Endpoints for FastAPI. 
"""

import os
import csv
import json
import requests
import shutil
import base64
import pandas as pd
import langchain
from doc_tools import parse_doc, extract_questions_answers, store_embeddings
from retrieval_pipeline import retrieve_similar_docs
from document_grader import document_grading
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone
from langchain.messages import SystemMessage, HumanMessage
from dotenv import load_dotenv, find_dotenv

_ = load_dotenv(find_dotenv())
openai_api_key = os.getenv("OPENAI_API_KEY")
upstage_api_key = os.getenv("UPSTAGE_API_KEY")
pinecone_api_key = os.getenv("PINECONE_API_KEY")
pc = Pinecone(api_key = pinecone_api_key)
embeddings = OpenAIEmbeddings(model="text-embedding-3-large", openai_api_key=openai_api_key)
llm = ChatOpenAI(model = "gpt-4o", temperature = 0)
df = pd.read_csv("./pdf_log.csv")


def generate_answer_for_query(query: str)-> dict:
    """
    Generates the final answer using the retrieved documents from the Vector Database

    Parameters
    ----------
    query: str
        The query sent by the user

    answer: dict
        The final answer generated by the GPT-4o model

    Returns
    -------
    dict
        The response to the user's query
        
    """
    
    
    retrieved_answers = retrieve_similar_docs(query)
    if(retrieved_answers != "No similar docs." and retrieved_answers != False):
        answers = ""
        relevant_docs = document_grading(query, retrieved_answers)
        if(relevant_docs == "There was an error with the Open AI API. Please try again."):
            return {
            "message": "There was an error with the Open AI API. Please try again."
            }
        if(relevant_docs == "There is no relevant information available for the given question."):
            return {
            "message": "There is no relevant information available for the given question."
            }
        if(len(relevant_docs) == 1):
            question, sep, answer = relevant_docs[0].partition("Answer: ")
            answers = answer
        if(len(relevant_docs) > 1):
            for doc in relevant_docs:
                question, sep, answer = doc.partition("Answer: ")
                answers = answers + answer + "\n" + "\n" 
    if(retrieved_answers == "No similar docs."):
        return {
        "message": "There is no available information on the question."
        }        
    if(retrieved_answers == False):
        return {
        "message": "We are unable to provide an answer at the moment. There was an error in the Pinecone API"
        }
    
    try:
        messages = [
            SystemMessage(
            content=[
                {
                "type": "text",    
                "text": "You are a Text Assistant. Your task is to edit the text as per the following instructions. Do not add new words or remove information in the text. Do not combine a set of paragraphs together that already exist in the text, keep them separate. Do not break a paragraph into two that already exists as a singular paragraph in the text. Do not combine a set of sentences together that already exist in the text, keep them separate. Do not break a sentence into two that already exists as a singular sentence in the text. Break a paragraph into bullet points(that exist in the paragraph), if the paragraph contains a list of bullet points. Break a sentence into bullet points(that exist in the sentence), if the sentence contains a list of bullet points. Do not add commas or semicolons where they are not present. Do not replace commas or semicolons where they are present. Fix spelling mistakes where they are present. Only remove the information, that is at the absolute end of the text, which is not a part of the text in general. If no changes are needed, generate the text as it exists."
                }
            ]
            ),
            HumanMessage(
            content=[
                {
                "type": "text",
                "text":  answers
                }
            ]
            )
        ]

        response = llm.invoke(messages)
        return {
        "message": response.content
        }
    except Exception as e:
        return {
        "message": "We are unable to provide an answer at the moment. There was an error in the OpenAI API."
        }


def upload_pdf_to_storage(path: str)-> dict:
    """
    Uploads a PDF file and stores it in a directory

    Parameters
    ----------
    path : str
        The file path set by Gradio for the PDF
    
    Returns
    -------
    dict
        The verification message providing the status of the document
    
    """
        

    print("Path")
    print(path)
    directory_path_to_save = "./uploaded_pdfdocs"
    json_output_dir = "./json_parsedoutputs"

    os.makedirs(directory_path_to_save, exist_ok = True)
    os.makedirs(json_output_dir, exist_ok = True)
    df = pd.read_csv("./pdf_log.csv")
    file_name = os.path.basename(path)
    print("File name")
    print(file_name)
    file_path = os.path.join(directory_path_to_save, file_name)
    json_file_path = os.path.join(json_output_dir, file_name.replace(".pdf", ".json"))
    if(os.path.exists(file_path)):
        embeddings_created = df.loc[df["uploaded_pdf_link"] == file_path, "embeddings_created"].squeeze()
        json_parsed_link_created = df.loc[df["uploaded_pdf_link"] == file_path, "parsed_json_link"].squeeze()
        qa_pairs_extracted = df.loc[df["uploaded_pdf_link"] == file_path, "questions_answers_extracted"].squeeze()
    
    if(os.path.exists(file_path) and isinstance(embeddings_created, str)):
        print("Condition 1")
        return {
        "message": "This file has already been uploaded. Please upload a new file."
        }
    if(os.path.exists(file_path) and pd.isna(json_parsed_link_created)):
        print("Condition 2")
        result = parse_doc(file_path)
        if(result == False):
            return {
            "message": "PDF not successfully processed. The Upstage API is not working."
            }
        else:
            qa_pairs = extract_questions_answers(json_file_path)
            if(qa_pairs == "There was an error in the Open AI API. Unable to parse PDF."):
                return {
                "message": "There was an error in the Open AI API. Unable to parse PDF."
                }
            else:
                result = store_embeddings(file_path, qa_pairs)
                if(result == False):
                    return {
                    "message": "There was an error in the Pinecone API. Unable to store PDF."
                    }
                else:
                    return {
                    "message": "PDF successfully stored in Vector Database."
                    }
    if(os.path.exists(file_path) and isinstance(json_parsed_link_created, str) and pd.isna(qa_pairs_extracted)):
        print("Condition 3")
        qa_pairs = extract_questions_answers(json_parsed_link_created)
        if(qa_pairs == "There was an error in the Open AI API. Unable to parse PDF."):
            return {
                    "message": "There was an error in the Open AI API. Unable to store PDF."
            }
        else:
            result = store_embeddings(file_path, qa_pairs)
            if(result == False):
                return {
                    "message": "There was an error in the Pinecone API. Unable to store PDF."
            }
            else:
                return {
                    "message": "PDF successfully stored in Vector Database."
            }
    if(os.path.exists(file_path) and isinstance(json_parsed_link_created, str) and isinstance(qa_pairs_extracted, str) and pd.isna(embeddings_created)):
        print("Condition 4")
        df_qa = pd.read_csv(qa_pairs_extracted)
        qa_pairs = {"questions": df_qa["question"].tolist(), "answers": df_qa["answer"].tolist()}
        result = store_embeddings(file_path, qa_pairs)
        if(result == False):
            return {
                    "message": "There was an error in the Pinecone API. Unable to store PDF."
            }
        else:                    
            return {
                    "message": "PDF successfully stored in Vector Database."
            }
    else:
        print("Condition 5")
        shutil.copy(path, file_path)
        new_row = {
        "uploaded_pdf_link": file_path,
        "parsed_json_link": None,
        "question_answers_extracted": None,
        "embeddings_created": None
        }
        # Convert dict → DataFrame with one row
        df_new = pd.DataFrame([new_row])
        # Append safely (don’t overwrite existing rows)
        df_new.to_csv("./pdf_log.csv", mode="a", index=False, header=not os.path.exists("./pdf_log.csv"))
        result = parse_doc(file_path)
        if(result == False):
            return {
                    "message": "PDF not successfully processed. The Upstage API is not working."
            }
        else:
            qa_pairs = extract_questions_answers(json_file_path)
            print(qa_pairs)
            if(qa_pairs == "There was an error in the Open AI API. Unable to parse PDF."):
                return {
                    "message": "There was an error in the Open AI API. Unable to parse PDF."
                }
            else:
                result = store_embeddings(file_path, qa_pairs)
                if(result == False):
                    return {
                    "message": "There was an error in the Pinecone API. Unable to store PDF."
                }
                else:
                    return {
                    "message": "PDF successfully stored in Vector Database."
                }
