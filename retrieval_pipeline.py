"""
The module comprises of the retrieval tool and answer refinement tool. 
"""

import os
import langchain
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone, PineconeException
from langchain.schema import SystemMessage, HumanMessage
import gradio as gr
from dotenv import load_dotenv, find_dotenv

_ = load_dotenv(find_dotenv())
openai_api_key = os.getenv("OPENAI_API_KEY")
upstage_api_key = os.getenv("UPSTAGE_API_KEY")
pinecone_api_key = os.getenv("PINECONE_API_KEY")
index_name = "faqsampleindex"
embeddings = OpenAIEmbeddings(model="text-embedding-3-large", openai_api_key=openai_api_key)
llm = ChatOpenAI(model = "gpt-4o", temperature = 0)


def retrieve_similar_docs(query: str)-> str:
    """
    Retrieves the question-answer pair with the highest similarity to the query sent by the user from the Pinecone Vector Database

    Parameters
    ----------
    query: str
        The query sent by the user

    similar_doc: str
        The document with the highest similarity
    
    """

    
    try:
        vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)
        similar_docs = vectorstore.similarity_search_with_score(query, k=1)

        if similar_docs == None:
            return "No similar docs."
    
        for doc, score in similar_docs:
            similar_doc = doc.page_content
    
        return similar_doc
    
    except Exception as e:
        return False


def refine_answer(query: str)-> str:
    """
    Refines the answer retrieved from the Vector Database and generates the final answer

    Parameters
    ----------
    query: str
        The query sent by the user

    answer: str
        The refined answer generated by the GPT-4o model
        
    """

    
    if(query == ""):
        return gr.update(value="Please provide a question.")
    
    retrieved_answer = retrieve_similar_docs(query)
    if(retrieved_answer != "No similar docs." and retrieved_answer != False):
        question, sep, answer = retrieved_answer.partition("Answer: ")
    if(retrieved_answer == "No similar docs."):
        return gr.update(value="There is no available information on the question.")
    if(retrieved_answer == False):
        return gr.update(value="We are unable to provide an answer at the moment. There was an error in the API.")

    messages = [
        SystemMessage(
        content=[
            {
              "type": "text",    
              "text": "You are a Text Assistant. Your task is to edit the text as per the following instructions. Do not add new words or remove information in the text. Do not combine a set of paragraphs together that already exist in the text, keep them separate. Do not break a paragraph into two that already exists as a singular paragraph in the text. Do not combine a set of sentences together that already exist in the text, keep them separate. Do not break a sentence into two that already exists as a singular sentence in the text. Break a paragraph into bullet points(that exist in the paragraph), if the paragraph contains a list of bullet points. Break a sentence into bullet points(that exist in the sentence), if the sentence contains a list of bullet points. Do not add commas or semicolons where they are not present. Do not replace commas or semicolons where they are present. Fix spelling mistakes where they are present. Only remove the information, that is at the absolute end of the text, which is not a part of the text in general. If no changes are needed, generate the text as it exists."
            }
        ]
        ),
        HumanMessage(
        content=[
            {
              "type": "text",
              "text":  answer
            }
        ]
        )
    ]

    try:
        response = llm.invoke(messages)
        return gr.update(value=response.content)
    except Exception as e:
        return gr.update(value="We are unable to provide an answer at the moment. There was an error in the API.")
